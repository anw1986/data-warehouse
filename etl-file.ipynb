{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 32-bit",
   "display_name": "Python 3.8.5 32-bit",
   "metadata": {
    "interpreter": {
     "hash": "f6edc2d87fdadcc5420f4e7ebc0e88850fbc7c09a6e68c5d2caeda0b56f7dde0"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dwh.cfg'))\n",
    "\n",
    "KEY                    = config.get('AWS','KEY')\n",
    "SECRET                 = config.get('AWS','SECRET')\n",
    "DWH_IAM_ROLE_NAME      = config.get(\"DWH\", \"DWH_IAM_ROLE_NAME\")\n",
    "\n",
    "# redshit cluster parameters\n",
    "DWH_CLUSTER_TYPE       = config.get(\"CLUSTER\",\"DB_CLUSTER_TYPE\")\n",
    "DWH_NUM_NODES          = config.get(\"CLUSTER\",\"DB_NUM_NODES\")\n",
    "DWH_NODE_TYPE          = config.get(\"CLUSTER\",\"DB_NODE_TYPE\")\n",
    "\n",
    "DWH_CLUSTER_IDENTIFIER = config.get(\"CLUSTER\",\"DB_CLUSTER_IDENTIFIER\")\n",
    "DWH_DB                 = config.get(\"CLUSTER\",\"DB_NAME\")\n",
    "DWH_DB_USER            = config.get(\"CLUSTER\",\"DB_USER\")\n",
    "DWH_DB_PASSWORD        = config.get(\"CLUSTER\",\"DB_PASSWORD\")\n",
    "DWH_PORT               = config.get(\"CLUSTER\",\"DB_PORT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\n",
    "    service_name='s3',\n",
    "    region_name='us-west-2',\n",
    "    aws_access_key_id=KEY,\n",
    "    aws_secret_access_key=SECRET\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampleDbBucket =  s3.Bucket(\"udacity-dend\")\n",
    "print(type(sampleDbBucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Print out bucket names\n",
    "for bucket in sampleDbBucket.objects.all():\n",
    "    print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "obj_list = list(sampleDbBucket.objects.filter(Prefix='song_data/A/B/C/'))\n",
    "obj = obj_list[0]\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_song_data = pd.read_json('s3://udacity-dend/song_data/A/B/C/TRABCAS128F14A25E2.json',  lines=True)\n",
    "df_song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = list(sampleDbBucket.objects.filter(Prefix='log_data'))\n",
    "obj = obj_list[1]\n",
    "obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data = pd.read_json('s3://udacity-dend/log_data/2018/11/2018-11-01-events.json',  lines=True)\n",
    "df_log_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')"
   ]
  },
  {
   "source": [
    "Create an IAM role, attach policy and get ARN (amazon resource name) programatically"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client('iam',aws_access_key_id=KEY,\n",
    "                     aws_secret_access_key=SECRET,\n",
    "                     region_name='us-west-2'\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "#1.1 Create the role, \n",
    "try:\n",
    "    print(\"1.1 Creating a new IAM Role\") \n",
    "    dwhRole = iam.create_role(\n",
    "        Path='/',\n",
    "        RoleName=DWH_IAM_ROLE_NAME,\n",
    "        Description = \"Allows Redshift clusters to call AWS services on your behalf.\",\n",
    "        AssumeRolePolicyDocument=json.dumps(\n",
    "            {'Statement': [{'Action': 'sts:AssumeRole',\n",
    "               'Effect': 'Allow',\n",
    "               'Principal': {'Service': 'redshift.amazonaws.com'}}],\n",
    "             'Version': '2012-10-17'})\n",
    "    )    \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    \n",
    "    \n",
    "print(\"1.2 Attaching Policy\")\n",
    "\n",
    "iam.attach_role_policy(RoleName=DWH_IAM_ROLE_NAME,\n",
    "                       PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\"\n",
    "                      )['ResponseMetadata']['HTTPStatusCode']\n",
    "\n",
    "print(\"1.3 Get the IAM role ARN\")\n",
    "roleArn = iam.get_role(RoleName=DWH_IAM_ROLE_NAME)['Role']['Arn']\n",
    "\n",
    "print(roleArn)"
   ]
  },
  {
   "source": [
    "Create Redshift cluster with parameters from the config file and Attach IAM role "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create redshit client\n",
    "redshift = boto3.client('redshift',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    response = redshift.create_cluster(        \n",
    "        #HW\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        NumberOfNodes=int(DWH_NUM_NODES),\n",
    "\n",
    "        #Identifiers & Credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        #Roles (for s3 access)\n",
    "        IamRoles=[roleArn]  \n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster parameters\n",
    "\n",
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth', -1)\n",
    "    keysToShow = [\"ClusterIdentifier\", \"NodeType\", \"ClusterStatus\", \"MasterUsername\", \"DBName\", \"Endpoint\", \"NumberOfNodes\", 'VpcId']\n",
    "    x = [(k, v) for k,v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"Key\", \"Value\"])\n",
    "\n",
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cluster end point and ARN\n",
    "\n",
    "DWH_ENDPOINT = myClusterProps['Endpoint']['Address']\n",
    "DWH_ROLE_ARN = myClusterProps['IamRoles'][0]['IamRoleArn']\n",
    "print(\"DWH_ENDPOINT :: \", DWH_ENDPOINT)\n",
    "print(\"DWH_ROLE_ARN :: \", DWH_ROLE_ARN)"
   ]
  },
  {
   "source": [
    "Open an incoming TCP port to access the cluster ednpoint"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2',\n",
    "                       region_name=\"us-west-2\",\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterProps['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    defaultSg.authorize_ingress(\n",
    "        # GroupName=defaultSg.group_name,\n",
    "        GroupName='default',\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "source": [
    "Connect to the RedShift Cluster"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "%load_ext sql"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conn_string=\"postgresql://{}:{}@{}:{}/{}\".format(DWH_DB_USER, DWH_DB_PASSWORD, DWH_ENDPOINT, DWH_PORT,DWH_DB)\n",
    "%store conn_string\n",
    "print(conn_string)\n",
    "%sql $conn_string"
   ]
  },
  {
   "source": [
    "create staging tables for song and log file. Use COPY command to get data from S3 to Redshift"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS \"staging_songs\";\n",
    "DROP TABLE IF EXISTS \"staging_events_log\";\n",
    "\n",
    "CREATE TABLE staging_songs(\n",
    "        num_songs           INTEGER,\n",
    "        artist_id           VARCHAR,\n",
    "        artist_latitude     FLOAT,\n",
    "        artist_longitude    FLOAT,\n",
    "        artist_location     VARCHAR,\n",
    "        artist_name         VARCHAR,\n",
    "        song_id             VARCHAR,\n",
    "        title               VARCHAR,\n",
    "        duration            FLOAT,\n",
    "        year                INTEGER\n",
    "    );\n",
    "\n",
    "CREATE TABLE staging_events_log(\n",
    "    artist                  VARCHAR,\n",
    "    auth                    VARCHAR,\n",
    "    firstName               VARCHAR,              \n",
    "    gender                  VARCHAR,\n",
    "    itemInSession           INTEGER,\n",
    "    lastName                VARCHAR,\n",
    "    length                  FLOAT,\n",
    "    level                   VARCHAR,\n",
    "    location                VARCHAR,\n",
    "    method                  VARCHAR,\n",
    "    page                    VARCHAR,\n",
    "    registration            FLOAT,\n",
    "    sessionId               INTEGER,\n",
    "    song                    VARCHAR,\n",
    "    status                  INTEGER,\n",
    "    ts                      TIMESTAMP,\n",
    "    userAgent               VARCHAR,\n",
    "    userId                  INTEGER\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy from S3 to songs staging table\n",
    "qry = \"\"\"\n",
    "    copy staging_songs \n",
    "    from {} \\\n",
    "    iam_role {}\n",
    "    json 'auto';\n",
    "\"\"\".format(config['S3']['SONG_DATA'],config['IAM_ROLE']['ARN'])\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM staging_songs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_songs LIMIT 10;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# copy from S3 to events log staging table\n",
    "qry = \"\"\"\n",
    "    copy staging_events_log \n",
    "    from {} \\\n",
    "    iam_role {}\\\n",
    "    FORMAT AS json {}\\\n",
    "    TIMEFORMAT 'epochmillisecs';\n",
    "\"\"\".format(config['S3']['LOG_DATA'],config['IAM_ROLE']['ARN'],config['S3']['LOG_JSONPATH'])\n",
    "\n",
    "%sql $qry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT count(*) FROM staging_events_log;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM staging_events_log LIMIT 5;"
   ]
  },
  {
   "source": [
    "Create Dimension and Fact Table. \n",
    "Get data from staging table to target table using INSERT/INSERT INTO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS \"song_table\";\n",
    "DROP TABLE IF EXISTS \"artist_table\";\n",
    "DROP TABLE IF EXISTS \"user_table\";\n",
    "DROP TABLE IF EXISTS \"time_table\";\n",
    "DROP TABLE IF EXISTS \"songplay_table\";\n",
    "\n",
    "CREATE TABLE song_table(\n",
    "    song_id     VARCHAR, \n",
    "    title       VARCHAR, \n",
    "    artist_id   VARCHAR, \n",
    "    year        INT,\n",
    "    duration    FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE artist_table(\n",
    "    artist_id   VARCHAR,\n",
    "    name        VARCHAR, \n",
    "    location    VARCHAR, \n",
    "    latitude    FLOAT, \n",
    "    longitude   FLOAT\n",
    ");\n",
    "\n",
    "CREATE TABLE user_table(\n",
    "    user_id     INTEGER, \n",
    "    first_name  VARCHAR, \n",
    "    last_name   VARCHAR, \n",
    "    gender      VARCHAR, \n",
    "    level       VARCHAR\n",
    ");\n",
    "\n",
    "CREATE TABLE time_table(\n",
    "    start_time      TIMESTAMP,\n",
    "    time_hour       INTEGER, \n",
    "    time_day        INTEGER, \n",
    "    time_week       INTEGER, \n",
    "    time_month      INTEGER, \n",
    "    time_year       INTEGER, \n",
    "    time_weekday    INTEGER\n",
    ");\n",
    "\n",
    "CREATE TABLE songplay_table(\n",
    "    songplay_id     INT IDENTITY(0,1), \n",
    "    start_time      VARCHAR, \n",
    "    user_id         INTEGER, \n",
    "    level           VARCHAR, \n",
    "    song_id         VARCHAR, \n",
    "    artist_id       VARCHAR, \n",
    "    session_id      INTEGER, \n",
    "    location        VARCHAR, \n",
    "    user_agent      VARCHAR\n",
    ");\n"
   ]
  },
  {
   "source": [
    "Test songplay query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT ts, userId, level, song_id, artist_id, sessionId, location, userAgent FROM staging_events_log se, staging_songs ss \n",
    "WHERE se.page='NextSong' \n",
    "AND se.song = ss.title\n",
    "AND se.artist = ss.artist_name\n",
    "AND se.length = ss.duration\n",
    "limit 5;"
   ]
  },
  {
   "source": [
    "Ingest from staging tables to target tables. Use INSERT INTO SELECT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO song_table SELECT song_id, title, artist_id, year, duration FROM staging_songs;\n",
    "\n",
    "INSERT INTO artist_table SELECT artist_id, artist_name, artist_location, artist_latitude, artist_longitude FROM staging_songs;\n",
    "\n",
    "INSERT INTO user_table SELECT userId, firstName, lastName, gender, level FROM staging_events_log;\n",
    "\n",
    "INSERT INTO time_table SELECT ts, EXTRACT(HOUR FROM ts), EXTRACT(DAY FROM ts), EXTRACT(WEEK FROM ts), EXTRACT(MONTH FROM ts), EXTRACT(YEAR FROM ts), EXTRACT(dayofweek FROM ts) FROM staging_events_log;\n",
    "\n",
    "INSERT INTO songplay_table (start_time, user_id, level, song_id, artist_id, session_id, location, user_agent) SELECT ts, userId, level, song_id, artist_id, sessionId, location, userAgent FROM staging_events_log se, staging_songs ss \n",
    "WHERE se.page='NextSong' \n",
    "AND se.song = ss.title\n",
    "AND se.artist = ss.artist_name\n",
    "AND se.length = ss.duration;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%sql SELECT * FROM time_table LIMIT 5;"
   ]
  },
  {
   "source": [
    "Delete redshit cluster (DO NOT run this during execution)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "redshift.delete_cluster( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,  SkipFinalClusterSnapshot=True)\n",
    "#### CAREFUL!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myClusterProps = redshift.describe_clusters(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterProps)"
   ]
  },
  {
   "source": [
    "Delete IAM role"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CAREFUL!!\n",
    "#-- Uncomment & run to delete the created resources\n",
    "# iam.detach_role_policy(RoleName=DWH_IAM_ROLE_NAME, PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\")\n",
    "# iam.delete_role(RoleName=DWH_IAM_ROLE_NAME)\n",
    "#### CAREFUL!!"
   ]
  }
 ]
}